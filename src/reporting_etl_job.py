# from pyspark.context import SparkContext
# from awsglue.context import GlueContext
# from awsglue.job import Job
# import pyspark.sql.functions as F
# from awsglue.utils import getResolvedOptions
# import sys
# try:
#     from awsglue.utils import getResolvedOptions
# except ImportError:
#     pass

# # 1. Defini√ß√£o do Contexto Glue e Argumentos
# sc = SparkContext.getOrCreate()
# glueContext = GlueContext(sc)
# spark = glueContext.spark_session
# job = Job(glueContext)

# # Pega os argumentos passados pelo Step Functions/Glue Job
# try:
#     # Tenta obter argumentos do Step Function
#     args = getResolvedOptions(sys.argv, ['JOB_NAME', 'datalake_bucket'])
#     S3_BUCKET_NAME = args['datalake_bucket']
# except Exception as e:
#     # Fallback para o caso de teste local ou erro (usar valor padr√£o)
#     print(f"ATEN√á√ÉO: N√£o foi poss√≠vel obter o argumento 'datalake_bucket'. Erro: {e}. Usando fallback...")
#     # ATEN√á√ÉO: Substituir pelo nome real do bucket se testado localmente.
#     S3_BUCKET_NAME = "nyc-taxi-data-lake-jha-case-ifood"
    
# # --- Vari√°veis de Caminho ---
# # Assumimos que o Glue Job anterior salvou os dados limpos neste caminho
# CONSUMER_PATH  = f"s3a://{S3_BUCKET_NAME}/consumer/trips_delta/" 
# REPORTING_PATH = f"s3a://{S3_BUCKET_NAME}/analytics/reporting/" 

# # 2. Leitura da Tabela Delta (Camada Consumer)
# print(f"Iniciando leitura da tabela Delta em: {CONSUMER_PATH}")

# # Leitura com suporte Delta Lake (configurado via default_arguments no Terraform)
# df_trips = spark.read.format("delta").load(CONSUMER_PATH)

# # --- 3. Fun√ß√£o de An√°lise e Reporting ---

# def run_analytics_reporting(df):
#     """
#     Executa as consultas de neg√≥cio (Q1 e Q2) e salva os resultados no S3 
#     (Camada Reporting), garantindo que as tabelas sejam acess√≠veis via Athena.
#     """
    
#     # Adicionando colunas de data/hora essenciais para agrega√ß√£o
#     # Usando os nomes de coluna padronizados ('pickup_datetime')
#     df_with_dates = df.withColumn("trip_year", F.year(F.col("pickup_datetime"))) \
#                       .withColumn("trip_month", F.month(F.col("pickup_datetime"))) \
#                       .withColumn("pickup_hour", F.hour(F.col("pickup_datetime")))
    
#     # ----------------------------------------------------------------------
#     # PERGUNTA 1: M√©dia de valor total (total_amount) por m√™s
#     # ----------------------------------------------------------------------
    
#     print("\n--- Processando Q1: M√©dia de Valor Total por M√™s ---")
    
#     # Cria a chave de agrupamento Ano-M√™s formatada (ex: 2023-01)
#     df_q1 = df_with_dates.withColumn(
#         "report_month", F.concat(F.col("trip_year"), F.lit("-"), F.format_string("%02d", F.col("trip_month")))
#     )

#     # Calcula a m√©dia mensal
#     df_result_q1 = df_q1.groupBy("report_month").agg(
#         F.avg("total_amount").alias("avg_total_amount")
#     ).orderBy("report_month")

#     # Salva o resultado no S3 Reporting (Pasta Q1)
#     output_path_q1 = f"{REPORTING_PATH}q1_monthly_revenue/"
#     print(f"Salvando resultados da Q1 em: {output_path_q1}")
#     df_result_q1.write.mode("overwrite").parquet(output_path_q1)


#     # ----------------------------------------------------------------------
#     # PERGUNTA 2: M√©dia de passageiros por cada hora do dia, por tipo de viagem
#     # ----------------------------------------------------------------------
    
#     print("\n--- Processando Q2: M√©dia de Passageiros por Hora e Tipo de Viagem ---")
    
#     # Agrupamento pela hora e c√°lculo da m√©dia de passageiros (incluindo o tipo de viagem)
#     df_result_q2 = df_with_dates.groupBy("pickup_hour", "trip_type").agg(
#         F.avg("passenger_count").alias("avg_passenger_count")
#     ).withColumnRenamed("pickup_hour", "report_hour") # Renomeia para o cat√°logo
    
#     # Salva o resultado no S3 Reporting (Pasta Q2)
#     output_path_q2 = f"{REPORTING_PATH}q2_hourly_passengers/"
#     print(f"Salvando resultados da Q2 em: {output_path_q2}")
#     df_result_q2.write.mode("overwrite").parquet(output_path_q2)

#     print("\n--- Relat√≥rios Q1 e Q2 criados na Camada Reporting. ---")
#     return True

# # 4. Orquestra√ß√£o e Execu√ß√£o
# if __name__ == '__main__':
    
#     df_trips_count = df_trips.count()
#     print(f"Registros lidos da Camada Consumer (Delta Lake): {df_trips_count}")

#     if df_trips_count == 0:
#         # üö® Ponto de Falha de Diagn√≥stico: Se 0, o job anterior falhou ao gravar.
#         print("WARNING: O DataFrame de entrada da Camada Consumer est√° vazio. Job finalizado sem gerar relat√≥rios.")
#         job.commit()
#         # Garante que o Glue Job termine como SUCESSO, j√° que n√£o havia dados para processar
#         sys.exit(0) 

#     # Se houver dados, executa a an√°lise
#     run_analytics_reporting(df_trips)
#     job.commit()

# Importa√ß√µes necess√°rias
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
import pyspark.sql.functions as F
from awsglue.utils import getResolvedOptions
import sys

# 1. Configura√ß√£o do Glue e Argumentos
sc = SparkContext.getOrCreate()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)

# --- Vari√°veis de Caminho ---
try:
    # O argumento 'datalake_bucket' √© necess√°rio para construir os caminhos S3
    args = getResolvedOptions(sys.argv, ['JOB_NAME', 'datalake_bucket'])
    S3_BUCKET_NAME = args['datalake_bucket']
except Exception:
    S3_BUCKET_NAME = "nyc-taxi-data-lake-jha-case-ifood"
    
# Caminhos das Duas Tabelas Delta na Camada Consumer
YELLOW_PATH = f"s3a://{S3_BUCKET_NAME}/consumer/trips_delta/yellow/"
GREEN_PATH = f"s3a://{S3_BUCKET_NAME}/consumer/trips_delta/green/"

# Caminhos de Escrita para a Camada Reporting (definidos nos arquivos reporting_tables.tf)
REPORTING_Q1_PATH = f"s3a://{S3_BUCKET_NAME}/analytics/reporting/q1_avg_total_amount_monthly/"
REPORTING_Q2_PATH = f"s3a://{S3_BUCKET_NAME}/analytics/reporting/q2_avg_passengers_hourly_may/"


def read_delta_table(s3_path):
    """Tenta ler uma tabela Delta Lake. Retorna um DataFrame vazio se o caminho for inv√°lido ou o DF estiver vazio."""
    print(f"Tentando ler Delta Table em: {s3_path}")
    try:
        # Tenta ler o Delta
        df = spark.read.format("delta").load(s3_path)
        count = df.count()
        if count == 0:
            print(f"AVISO: DataFrame lido de {s3_path} est√° vazio.")
            return None
        print(f"Sucesso: {count} registros lidos de {s3_path}.")
        return df
    except Exception as e:
        print(f"ERRO ao ler {s3_path}. O caminho ou a tabela podem n√£o existir: {e}")
        return None

def analyze_q1(df_yellow):
    """
    Relat√≥rio Q1: Qual a m√©dia de valor total (total_amount) recebido em um m√™s 
    considerando todos os yellow t√°xis da frota?
    """
    if df_yellow is None:
        return None
        
    print("-" * 50)
    print("Iniciando An√°lise Q1 (Yellow Taxis)")
    print("-" * 50)

    # 1. Agrega√ß√£o: Agrupa por ano/m√™s e calcula a m√©dia do total_amount
    df_q1 = df_yellow.groupBy(F.col("trip_year"), F.col("trip_month")) \
                     .agg(F.avg("total_amount").alias("avg_total_amount"))
    
    # 2. Formata√ß√£o: Cria a coluna report_month no formato YYYY-MM
    df_q1 = df_q1.withColumn("report_month", 
                             F.concat_ws("-", F.col("trip_year"), F.lpad(F.col("trip_month"), 2, "0"))) \
                 .select("report_month", "avg_total_amount")
    
    # 3. Escrita: Salva o resultado no Reporting Layer
    print(f"Escrevendo Q1 em: {REPORTING_Q1_PATH}")
    df_q1.write.mode("overwrite").parquet(REPORTING_Q1_PATH)
    
    df_q1.show()
    return df_q1

def analyze_q2(df_yellow, df_green):
    """
    Relat√≥rio Q2: Qual a m√©dia de passageiros (passenger_count) por cada hora do dia
    que pegaram t√°xi no m√™s de maio considerando todos os t√°xis da frota?
    """
    # Verifica se h√° dados dispon√≠veis para a uni√£o
    if df_yellow is None and df_green is None:
        return None
        
    print("-" * 50)
    print("Iniciando An√°lise Q2 (Todos os Taxis)")
    print("-" * 50)

    # 1. Uni√£o: Consolida os dados Yellow e Green (se houver)
    if df_yellow is None:
        df_all = df_green
    elif df_green is None:
        df_all = df_yellow
    else:
        df_all = df_yellow.unionByName(df_green)
        
    # 2. Filtragem e Feature Engineering: Filtra por Maio (M√™s 5) e extrai a hora
    # O campo 'trip_month' √© um inteiro (1 a 12)
    df_q2 = df_all.filter(F.col("trip_month") == 5) 
    
    # Ponto de checagem para garantir que h√° dados em Maio
    if df_q2.count() == 0:
        print("AVISO: Nenhum dado de Maio (M√™s 5) encontrado ap√≥s a uni√£o. Relat√≥rio Q2 ser√° vazio.")
        # Cria um DF vazio com o schema correto
        empty_schema = ["report_hour", "avg_passenger_count", "trip_type"]
        empty_df = spark.createDataFrame([], empty_schema)
        empty_df.write.mode("overwrite").parquet(REPORTING_Q2_PATH)
        return empty_df

    df_q2 = df_q2.withColumn("report_hour", F.hour(F.col("pickup_datetime")))

    # 3. Agrega√ß√£o: Agrupa por hora e tipo de t√°xi (para visualiza√ß√£o)
    df_q2 = df_q2.groupBy(F.col("report_hour"), F.col("trip_type")) \
                 .agg(F.avg("passenger_count").alias("avg_passenger_count")) \
                 .select("report_hour", "avg_passenger_count", "trip_type")
    
    # 4. Escrita: Salva o resultado no Reporting Layer
    print(f"Escrevendo Q2 em: {REPORTING_Q2_PATH}")
    df_q2.write.mode("overwrite").parquet(REPORTING_Q2_PATH)
    
    df_q2.show()
    return df_q2

# 2. Orquestra√ß√£o e Execu√ß√£o
if __name__ == '__main__':
    
    # Lendo os DataFrames da Camada Consumer
    df_yellow = read_delta_table(YELLOW_PATH)
    df_green = read_delta_table(GREEN_PATH)
    
    # Executa a an√°lise Q1 (apenas Yellow)
    analyze_q1(df_yellow)
    
    # Executa a an√°lise Q2 (Yellow + Green)
    analyze_q2(df_yellow, df_green)
    
    job.commit()
