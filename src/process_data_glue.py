from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
import pyspark.sql.functions as F
from pyspark.sql.window import Window # üö® NOVO: Para a l√≥gica de ranqueamento

# 1. Configura√ß√£o do Glue
# O Glue automaticamente configura o S3, eliminando a necessidade de chaves!

# --- Vari√°veis de Caminho ---
# O nome do bucket deve ser inferido do ambiente (melhor pr√°tica), mas usaremos o valor para o caminho
S3_BUCKET_NAME = "nyc-taxi-data-lake-jha-case-ifood"
LANDING_PATH   = f"s3a://{S3_BUCKET_NAME}/landing/" # Lemos do root da landing
CONSUMER_PATH  = f"s3a://{S3_BUCKET_NAME}/consumer/trips_delta/" # üö® NOVO: Caminho para tabela Delta

# Colunas OBRIGAT√ìRIAS (mantidas)
COLUMNS_MAPPING = {
    "VendorID": "vendor_id",
    "passenger_count": "passenger_count",
    "total_amount": "total_amount",
    "tpep_pickup_datetime": "pickup_datetime",
    "tpep_dropoff_datetime": "dropoff_datetime",
}

# 2. Defini√ß√£o do Contexto Glue
sc = SparkContext.getOrCreate()
glueContext = GlueContext(sc)
spark = glueContext.spark_session 
job = Job(glueContext)

# 3. L√≥gica de Fun√ß√£o (Mantida)
def apply_data_quality_and_transform(df):
    """Aplica transforma√ß√µes e regras de Data Quality (DQ)."""
    
    print(f"N√∫mero de registros brutos (√∫ltima vers√£o) antes do DQ: {df.count()}")
    
    # 1. Sele√ß√£o e Renomea√ß√£o de Colunas
    df_transformed = df.select([F.col(k).alias(v) for k, v in COLUMNS_MAPPING.items()])
    
    # 2. Regras de Data Quality (DQ)
    df_dq = df_transformed.filter(F.col("passenger_count").isNotNull() & (F.col("passenger_count") > 0))
    df_dq = df_dq.filter(F.col("total_amount").isNotNull() & (F.col("total_amount") > 0))
    df_dq = df_dq.filter(F.col("pickup_datetime").isNotNull() & F.col("dropoff_datetime").isNotNull())

    # 3. Engenharia de Features (Cria√ß√£o de Parti√ß√£o)
    df_dq = df_dq.withColumn("trip_year", F.year(F.col("pickup_datetime")))
    df_dq = df_dq.withColumn("trip_month", F.month(F.col("pickup_datetime")))
    
    print(f"N√∫mero de registros ap√≥s o DQ: {df_dq.count()}")
    return df_dq

# 4. Orquestra√ß√£o e Execu√ß√£o
if __name__ == '__main__':
    
    # üö® NOVO: Leitura e Sele√ß√£o da √öltima Vers√£o
    
    # O Spark/Glue infere as colunas de parti√ß√£o: trip_type, partition_date, ingest_ts
    print(f"Iniciando leitura de todas as vers√µes da Landing Zone em: {LANDING_PATH}")
    df_landing = spark.read.parquet(LANDING_PATH)

    # 4.1. Definir a Window Function (Particionar por m√™s/tipo e ordenar por TS)
    # Colunas de parti√ß√£o impl√≠citas: 'trip_type', 'partition_date' (que √© YYYYMM)
    window_spec = Window.partitionBy("trip_type", "partition_date")\
                        .orderBy(F.col("ingest_ts").desc())

    # 4.2. Aplicar ranqueamento (o TS mais novo recebe rank = 1)
    df_ranked = df_landing.withColumn("rank", F.rank().over(window_spec))

    # 4.3. Filtrar apenas o rank 1 (√∫ltima vers√£o ingerida) e remover a coluna de ranqueamento
    df_latest = df_ranked.filter(F.col("rank") == 1).drop("rank")
    
    print(f"Registros filtrados (apenas √∫ltima vers√£o): {df_latest.count()}")

    # 5. Transforma√ß√£o e DQ
    df_consumer = apply_data_quality_and_transform(df_latest)

    # 6. Escrita em Delta Lake (Sink)
    print(f"Iniciando escrita da camada Consumer (Delta Lake) em: {CONSUMER_PATH}")
    
    # üö® NOVO SINK: Uso do formato 'delta'
    # NOTA: O modo 'overwrite' substitui APENAS as parti√ß√µes modificadas no Delta Lake
    # quando a instru√ß√£o √© particionada, garantindo o "Overwrite Partition".
    df_consumer.write.mode("overwrite") \
             .format("delta") \
             .partitionBy("trip_year", "trip_month") \
             .save(CONSUMER_PATH)

    print("Escrita em Delta Lake conclu√≠da.")
    job.commit()