# üöï Desafio T√©cnico: Engenharia de Dados NYC Taxi Trips

Este reposit√≥rio cont√©m uma solu√ß√£o completa de Engenharia de Dados para ingest√£o, processamento e an√°lise dos dados de viagens de t√°xi de Nova York, conforme proposto no desafio t√©cnico.

A solu√ß√£o √© constru√≠da usando uma stack **Serverless e Cloud-Native (AWS)** e um motor de processamento moderno (**AWS Glue Serverless/PySpark**), demonstrando habilidades em IaC, ETL/ELT e Data Quality.

-----

## üöÄ Stack Tecnol√≥gica Escolhida

| Etapa do Pipeline | Tecnologia/Servi√ßo | Finalidade no Projeto |
| :--- | :--- | :--- |
| **Orquestra√ß√£o** | **AWS Step Functions** | **Controlador Serverless.** Gerencia o *workflow* completo, garantindo que a Ingest√£o (Lambda) termine antes que o Processamento (Glue) comece. |
| **Infraestrutura como C√≥digo (IaC)** | **Terraform** | Provisionamento seguro e automatizado do S3, Glue Job, Lambda, Step Functions, IAM Roles e Athena. |
| **Data Lake e Armazenamento** | **Amazon S3** | Armazenamento persistente de dados brutos (`/landing`) e processados (`/consumer`) em Parquet. |
| **Ingest√£o (EL)** | **AWS Lambda (Python/`requests`/`boto3`)** | **Fun√ß√£o Serverless.** Baixa os arquivos e faz o upload direto para a Landing Zone, com caminho particionado por *timestamp* de ingest√£o. |
| **Processamento e Transforma√ß√£o (T)** | **AWS Glue Job (PySpark)** | **Motor ETL Serverless da AWS.** L√™ a √∫ltima vers√£o dos dados (pelo *timestamp*), aplica transforma√ß√µes e salva na Camada de Consumo. |
| **Cat√°logo de Dados** | **AWS Glue Data Catalog** | Registro do *schema* da tabela de consumo para ser acessada via SQL. |
| **Disponibiliza√ß√£o (SQL)** | **Amazon Athena** | Permite consultas SQL diretas sobre os dados Parquet do S3. |
| **An√°lise e Relat√≥rios** | **Jupyter Notebook** (+ Pandas/Matplotlib) | Realiza√ß√£o das an√°lises solicitadas e visualiza√ß√£o dos resultados. |

-----

## üéØ Objetivo do Desafio

Desenvolver um pipeline ELT completo (Ingest√£o $\rightarrow$ Processamento $\rightarrow$ Disponibiliza√ß√£o $\rightarrow$ An√°lise) para os dados de viagens de t√°xi de **Janeiro a Maio de 2023** da cidade de Nova York, garantindo que as colunas obrigat√≥rias (**VendorID**, **passenger\_count**, **total\_amount**, **tpep\_pickup\_datetime**, **tpep\_dropoff\_datetime**) estejam presentes e modeladas na camada de consumo.

-----

## 1\. ‚öôÔ∏è Fase 1: Provisionamento e Ingest√£o de Dados (EL)

Esta fase inicial cria toda a infraestrutura de Data Lake e a fun√ß√£o AWS Lambda para ingest√£o via Terraform.

### 1.1. Pr√©-requisitos e Setup Local

Certifique-se de que voc√™ possui as seguintes ferramentas instaladas e configuradas:

1.  **Python 3.x** e `pip`.
2.  **Terraform CLI**.
3.  **AWS CLI** configurado (via `aws configure`) com credenciais que tenham permiss√µes de `Admin` ou as pol√≠ticas espec√≠ficas para S3, Glue, Lambda, Step Functions e IAM.

### Instala√ß√£o de Depend√™ncias Python

O setup local √© m√≠nimo, pois o processamento principal ocorre na nuvem. Apenas as bibliotecas para *An√°lise* s√£o necess√°rias.

```bash
pip3 install pandas matplotlib
````

### 1.2. Provisionamento da Infraestrutura com Terraform (IaC)

O c√≥digo Terraform (localizado na pasta `infra/`) cria todos os recursos necess√°rios, incluindo o **AWS Step Functions** para orquestra√ß√£o.

**NOTA SOBRE O ESTADO:** Para simplificar a execu√ß√£o do avaliador, o estado do Terraform (`terraform.tfstate`) ser√° armazenado **localmente** na pasta `infra/`.

**ATEN√á√ÉO:** Edite o arquivo `main.tf` e substitua as *placeholders* pelo nome √∫nico do seu bucket S3 antes de prosseguir.

```bash
# Navega para a pasta do Terraform
cd infra/

# 1. Inicializa o ambiente Terraform (o estado local ser√° criado)
terraform init

# 2. Visualiza o plano de execu√ß√£o (recursos a serem criados)
terraform plan

# 3. Aplica o plano, criando toda a infraestrutura na AWS
terraform apply --auto-approve
```

### 1.3. Execu√ß√£o da Ingest√£o e Orquestra√ß√£o (ELT)

A execu√ß√£o do *pipeline* completo de ELT agora √© feita via o orquestrador **AWS Step Functions**, garantindo o fluxo: **Ingest√£o (Lambda) $\rightarrow$ Processamento (Glue Job)**.

> **NOTA SOBRE IDEMPOT√äNCIA:** A Landing Zone agora salva os dados com um *timestamp* de ingest√£o (`ingest_ts`) no caminho S3. O Glue Job ser√° configurado para **sempre processar a vers√£o mais recente** para cada m√™s, garantindo que re-execu√ß√µes n√£o causem duplica√ß√£o e que a camada de consumo seja baseada nos dados mais frescos.

#### 1.3.1. Execu√ß√£o Padr√£o (Valores Iniciais do Desafio)

O Step Function aceita um *payload* JSON que √© repassado ao Lambda, permitindo o controle de quais meses/anos ser√£o ingeridos. A invoca√ß√£o sem um *payload* espec√≠fico usar√° os valores *default* (Janeiro a Maio de 2023).

```bash
# O ARN da State Machine pode ser obtido na sa√≠da do Terraform, ex:
STATE_MACHINE_ARN="arn:aws:states:us-east-1:123456789012:stateMachine:nyc-taxi-elt-pipeline" 

# Invoca o Step Function, passando um payload vazio ({}) para usar defaults
aws stepfunctions start-execution \
    --state-machine-arn $STATE_MACHINE_ARN \
    --name "Run-$(date +%Y%m%d%H%M%S)" \
    --input '{}'
```

#### 1.3.2. Execu√ß√£o Customizada (Exemplo: Julho de 2024, Apenas Yellow)

Para ingerir dados de um per√≠odo diferente, passe um objeto JSON no campo `--input`:

```bash
# Payload para customizar a execu√ß√£o: ano 2024, m√™s 07, apenas yellow
INPUT='{"year": "2024", "months": ["07"], "trip_types": ["yellow"]}'

aws stepfunctions start-execution \
    --state-machine-arn $STATE_MACHINE_ARN \
    --name "Run-July2024-$(date +%Y%m%d%H%M%S)" \
    --input "$INPUT"
```

#### Sa√≠da Esperada (Verifica√ß√£o)

Verifique o Console do **AWS Step Functions** para visualizar o gr√°fico de execu√ß√£o em tempo real. O sucesso da execu√ß√£o indica que todo o pipeline (Lambda e Glue Job) foi conclu√≠do.

### ‚ö†Ô∏è Solu√ß√£o de Problemas Comuns (Troubleshooting)

Se, ao rodar o `terraform apply`, voc√™ encontrar o erro `AccessDenied` (C√≥digo 403) na cria√ß√£o de qualquer recurso AWS, isso indica que as credenciais AWS configuradas n√£o possuem as permiss√µes necess√°rias.

**A√ß√£o:** O usu√°rio IAM configurado via `aws configure` **deve ter permiss√µes de administrador** ou, no m√≠nimo, as pol√≠ticas espec√≠ficas para **S3, Glue, Lambda, Step Functions e IAM** anexadas. Tente rodar o `terraform apply --auto-approve` novamente ap√≥s corrigir as permiss√µes no Console AWS/IAM.

-----

## 2\. üß© Fase 2: Processamento e Data Quality (T)

Nesta fase, o c√≥digo PySpark processa os dados da Landing Zone, aplica transforma√ß√µes, garante a Qualidade de Dados (DQ) e salva o resultado na **Camada de Consumo**.

### 2.1. L√≥gica do PySpark (Sele√ß√£o da √öltima Vers√£o)

O script `process_data_glue.py` deve conter a l√≥gica para identificar e processar a vers√£o mais recente dos dados:

1.  Ler todos os dados da Landing Zone.
2.  Usar fun√ß√µes do Spark SQL ou `Window Functions` para identificar o valor **m√°ximo** do campo `ingest_ts` para cada `partition_date`.
3.  Filtrar o *DataFrame* para reter apenas as linhas correspondentes ao √∫ltimo `ingest_ts`.
4.  Aplicar transforma√ß√µes ETL e regras de Data Quality.

### 2.2. Execu√ß√£o do Job AWS Glue

O Job ETL ser√° iniciado automaticamente pelo Step Functions.

**Aten√ß√£o ao Custo:** O Job Glue est√° configurado com `Worker Type: G.025X` e `Number of Workers: 2`. O custo por execu√ß√£o √© **baixo** (pagamento por segundo), mas √© um recurso pago pela AWS.

**Pr√≥ximos Passos:**

1.  Finalize o script `process_data_glue.py` com a l√≥gica de sele√ß√£o da √∫ltima vers√£o.
2.  **Rode o `terraform apply`** (se ainda n√£o o fez) para garantir que a vers√£o mais recente do script foi enviada para o S3.

-----

## 3\. üîç Fase 3: An√°lise e Consumo de Dados

Nesta fase, os dados s√£o acessados via SQL (Athena) e analisados em um Notebook.

1.  **Verifica√ß√£o no Athena:** No Console da AWS, use o **Amazon Athena** para consultar a tabela `trips_consumer` criada pelo Terraform.
    ```sql
    SELECT count(*) FROM "nyc_taxi_db"."trips_consumer";
    ```
2.  **An√°lise no Notebook:** Abra o Jupyter e execute o Notebook (`analysis.ipynb`) que se conecta ao Athena (via PyAthena ou similar) ou l√™ o Parquet final do S3 para gerar as visualiza√ß√µes e *insights* solicitados.

-----

## üóëÔ∏è Limpeza de Recursos (Obrigat√≥rio)

Para evitar custos indesejados na sua conta AWS, **SEMPRE** destrua a infraestrutura ap√≥s o t√©rmino da avalia√ß√£o.

```bash
cd infra/
terraform destroy --auto-approve
```

-----

## üß† Metodologia e Produtividade

Este projeto foi desenvolvido utilizando uma abordagem de **Engenharia Aumentada por IA (AI-Augmented Engineering)**. O modelo de linguagem **Gemini (Google)** foi utilizado como um *pair programmer* arquitetural para:

1.  **Orquestra√ß√£o Serverless:** Implementar o **AWS Step Functions** como orquestrador central, elevando o pipeline a um n√≠vel profissional com monitoramento e controle de fluxo.
2.  **Idempot√™ncia e Versionamento:** Adotar a estrat√©gia de *timestamp* na Landing Zone para garantir a imutabilidade do dado *raw* e a leitura da √∫ltima vers√£o pelo processamento ETL.
3.  **Otimiza√ß√£o de Custos e Seguran√ßa:** Orienta√ß√£o na configura√ß√£o eficiente dos recursos (Lambda, Glue Workers) e uso de IAM Roles, mantendo o custo AWS baixo e a seguran√ßa em alta.

Esta metodologia visa demonstrar profici√™ncia em ferramentas de IA para **ganho de produtividade**, mantendo total controle e entendimento das decis√µes arquiteturais tomadas.
