# üöï Desafio T√©cnico: Engenharia de Dados NYC Taxi Trips

Este reposit√≥rio cont√©m uma solu√ß√£o completa de Engenharia de Dados para ingest√£o, processamento e an√°lise dos dados de viagens de t√°xi de Nova York, conforme proposto no desafio t√©cnico.

A solu√ß√£o √© constru√≠da usando uma stack **Serverless e Cloud-Native (AWS)** e um motor de processamento moderno (**PySpark/Databricks CE**), demonstrando habilidades em IaC, ETL/ELT e Data Quality.

-----

## üöÄ Stack Tecnol√≥gica Escolhida

| Etapa do Pipeline | Tecnologia/Servi√ßo | Finalidade no Projeto |
| :--- | :--- | :--- |
| **Infraestrutura como C√≥digo (IaC)** | **Terraform** | Provisionamento seguro e automatizado do S3 e AWS Glue/Athena. |
| **Data Lake e Armazenamento** | **Amazon S3** | Armazenamento persistente de dados brutos (`/landing`) e processados (`/consumer`) em Parquet. |
| **Ingest√£o (EL)** | **Python (`requests` e `boto3`)** | Solu√ß√£o de software para baixar os arquivos Parquet da TLC e carreg√°-los no S3. |
| **Processamento e Transforma√ß√£o (T)** | **PySpark** (Executado no Databricks CE ou local) | Leitura dos dados, filtragem de colunas, limpeza (Data Quality) e escrita no formato Parquet na Camada de Consumo. |
| **Cat√°logo de Dados** | **AWS Glue Data Catalog** | Registro do *schema* da tabela de consumo para ser acessada via SQL. |
| **Disponibiliza√ß√£o (SQL)** | **Amazon Athena** | Permite consultas SQL diretas sobre os dados Parquet do S3. |
| **An√°lise e Relat√≥rios** | **Jupyter Notebook** (+ Pandas/Matplotlib) | Realiza√ß√£o das an√°lises solicitadas e visualiza√ß√£o dos resultados. |

-----

## üéØ Objetivo do Desafio

Desenvolver um pipeline ELT completo (Ingest√£o $\rightarrow$ Processamento $\rightarrow$ Disponibiliza√ß√£o $\rightarrow$ An√°lise) para os dados de viagens de t√°xi de **Janeiro a Maio de 2023** da cidade de Nova York, garantindo que as colunas obrigat√≥rias (**VendorID**, **passenger\_count**, **total\_amount**, **tpep\_pickup\_datetime**, **tpep\_dropoff\_datetime**) estejam presentes e modeladas na camada de consumo.

-----

## 1\. ‚öôÔ∏è Fase 1: Provisionamento e Ingest√£o de Dados (EL)

Esta fase inicial cria a infraestrutura de Data Lake via Terraform e carrega os dados brutos.

### 1.1. Pr√©-requisitos e Setup Local

Certifique-se de que voc√™ possui as seguintes ferramentas instaladas e configuradas:

1.  **Python 3.x** e `pip`.
2.  **Terraform CLI**.
3.  **AWS CLI** configurado (via `aws configure`) com credenciais que tenham permiss√µes de `Admin` ou as pol√≠ticas espec√≠ficas para S3, Glue e IAM.

### Instala√ß√£o de Depend√™ncias Python

```bash
pip3 install requests boto3 pyspark pandas
```

### 1.2. Provisionamento da Infraestrutura com Terraform (IaC)

O c√≥digo Terraform (localizado na pasta `infra/`) cria o **Bucket S3** (nosso Data Lake) e o **AWS Glue Data Catalog**.

**NOTA SOBRE O ESTADO:** Para simplificar a execu√ß√£o do avaliador, o estado do Terraform (`terraform.tfstate`) ser√° armazenado **localmente** na pasta `infra/`.

**ATEN√á√ÉO:** Edite o arquivo `main.tf` e substitua as *placeholders* pelo nome √∫nico do seu bucket S3 antes de prosseguir.

```bash
# Navega para a pasta do Terraform
cd infra/

# 1. Inicializa o ambiente Terraform (o estado local ser√° criado)
terraform init

# 2. Visualiza o plano de execu√ß√£o (recursos a serem criados)
terraform plan

# 3. Aplica o plano, criando o S3 Bucket, Glue Database e Tabela Athena
terraform apply --auto-approve

# Volta para a raiz do projeto e acessa a pasta src (para rodar o script Python)
cd ../src
```

### 1.3. Execu√ß√£o da Ingest√£o (Carga no S3)

O script Python `ingest_data.py` √© a nossa **solu√ß√£o de ingest√£o**. Ele automatiza a extra√ß√£o (E) dos arquivos Parquet de Jan a Mai/2023 (Yellow e Green Taxis) do CloudFront da TLC e o upload (L) para a **Landing Zone** do S3.

**Pr√©-requisito:** Certifique-se de que a vari√°vel `S3_BUCKET_NAME` no `ingest_data.py` foi atualizada com o nome do bucket que o Terraform criou.

```bash
# Executa o script de Ingest√£o Python
python3 ingest_data.py
```

#### Sa√≠da Esperada

O script ir√° confirmar o upload para os 10 arquivos (5 meses x 2 tipos):

```
--- Iniciando Ingest√£o de Dados (2023) ---
Bucket de Destino: [SEU_BUCKET_NAME]
...
--- Processando 1/10 - Tipo: yellow, M√™s: 01 ---
-> Iniciando download: [URL_DO_CLOUDFRONT]
   [SUCESSO] Upload conclu√≠do para s3://[SEU_BUCKET_NAME]/landing/yellow_tripdata_2023-01.parquet
...
--- Ingest√£o Conclu√≠da! ---
```

### ‚ö†Ô∏è Solu√ß√£o de Problemas Comuns (Troubleshooting)

Se, ao rodar o `terraform apply`, voc√™ encontrar o erro `AccessDenied` (C√≥digo 403) na a√ß√£o `s3:CreateBucket` (ou qualquer a√ß√£o IAM, como `glue:*`), isso indica que as credenciais AWS configuradas n√£o possuem as permiss√µes necess√°rias para criar os recursos.

**Como Resolver:**

O usu√°rio IAM configurado via `aws configure` **deve ter permiss√µes de administrador** ou, no m√≠nimo, uma pol√≠tica anexada que permita as seguintes a√ß√µes:

* **S3:** `s3:*` (Para criar e gerenciar o bucket de dados)
* **Glue:** `glue:*` (Para criar o Glue Database e a tabela)
* **IAM:** (Se houver *Roles* criadas), `iam:PassRole`

**A√ß√£o:**

1.  Acesse o **Console da AWS** com um usu√°rio Administrador.
2.  V√° para o servi√ßo **IAM** e encontre o usu√°rio que est√° sendo usado no CLI.
3.  Anexe a pol√≠tica gerenciada pela AWS chamada **`AdministratorAccess`** ou uma pol√≠tica personalizada que contenha as a√ß√µes listadas acima.
4.  Ap√≥s a anexa√ß√£o, tente rodar o `terraform apply --auto-approve` novamente.

-----

## 2\. üß© Fase 2: Processamento e Data Quality (T)

Nesta fase, o c√≥digo PySpark processa os dados da Landing Zone, aplica transforma√ß√µes, garante a Qualidade de Dados (DQ) e salva o resultado na **Camada de Consumo**.

*(Neste ponto, voc√™ deve criar o script PySpark e o c√≥digo de Data Quality.)*

**Pr√≥ximos Passos:**

1.  Crie o script `process_data.py` (c√≥digo PySpark).
2.  O script deve ler de `s3://[SEU_BUCKET_NAME]/landing/`.
3.  Filtre as 5 colunas obrigat√≥rias e aplique as regras de DQ (ex: `passenger_count > 0`).
4.  Escreva o resultado em Parquet na Camada de Consumo: `s3://[SEU_BUCKET_NAME]/consumer/trips/`.

<!-- end list -->

```bash
# Exemplo de execu√ß√£o (utilizando spark-submit para rodar o PySpark localmente)
spark-submit process_data.py
```

-----

## 3\. üîç Fase 3: An√°lise e Consumo de Dados

Nesta fase, os dados s√£o acessados via SQL (Athena) e analisados em um Notebook.

1.  **Verifica√ß√£o no Athena:** No Console da AWS, use o **Amazon Athena** para consultar a tabela `trips_consumer` criada pelo Terraform.
    ```sql
    SELECT count(*) FROM "nyc_taxi_db"."trips_consumer";
    ```
2.  **An√°lise no Notebook:** Abra o Jupyter e execute o Notebook (`analysis.ipynb`) que se conecta ao Athena (via PyAthena ou similar) ou l√™ o Parquet final do S3 para gerar as visualiza√ß√µes e *insights* solicitados.

-----

## üóëÔ∏è Limpeza de Recursos (Obrigat√≥rio)

Para evitar custos indesejados na sua conta AWS, **SEM-PRE** destrua a infraestrutura ap√≥s o t√©rmino da avalia√ß√£o.

```bash
cd infra/
terraform destroy --auto-approve
```

-----
## üß† Metodologia e Produtividade

Este projeto foi desenvolvido utilizando uma abordagem de **Engenharia Aumentada por IA (AI-Augmented Engineering)**. O modelo de linguagem **Gemini (Google)** foi utilizado como um *pair programmer* arquitetural para:

1.  **Valida√ß√£o de Escolhas Tecnol√≥gicas:** Confirmar a escolha do Terraform sobre o CloudFormation para portabilidade de dados (Databricks CE).
2.  **Otimiza√ß√£o de Custos e Seguran√ßa:** Orienta√ß√£o na escolha da arquitetura *Serverless* (S3, Glue, Lambda) para manter o custo AWS pr√≥ximo de zero (`terraform destroy`).
3.  **Gera√ß√£o de C√≥digo Boilerplate:** Acelerar a cria√ß√£o de templates IaC (Terraform) e scripts de ingest√£o (Python/Boto3), permitindo que o foco principal fosse direcionado para a **L√≥gica PySpark e Qualidade de Dados**.

Esta metodologia visa demonstrar profici√™ncia em ferramentas de IA para **ganho de produtividade**, mantendo total controle e entendimento das decis√µes arquiteturais tomadas.

-----
## üí° Ferramentas de Produtividade

A arquitetura e as decis√µes de implementa√ß√£o neste projeto foram validadas e aceleradas com o aux√≠lio do **Gemini (Modelo de Linguagem do Google)**, utilizado como um acelerador de produtividade na fase de design de software e IaC. O foco em *best practices* foi mantido como prioridade.
